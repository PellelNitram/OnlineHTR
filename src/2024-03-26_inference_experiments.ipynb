{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchmetrics.functional.text import word_error_rate\n",
    "from torchmetrics.functional.text import char_error_rate\n",
    "\n",
    "from src.models.carbune_module import CarbuneLitModule2\n",
    "from src.models.carbune_module import LitModule1\n",
    "from src.models.components.carbune2020_net import Carbune2020NetAttempt1\n",
    "from src.utils.io import load_alphabet\n",
    "from src.data.tokenisers import AlphabetMapper\n",
    "from src.data.online_handwriting_datamodule import IAMOnDBDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../logs/train/multiruns/2024-03-20_13-11-14/0/tensorboard/version_0/checkpoints/epoch=9999-step=1900000.ckpt'\n",
    "PATH = '../logs/train/multiruns/2024-03-27_14-06-58/0/checkpoints/epoch_epoch=029.ckpt'\n",
    "\n",
    "BASE_PATH = Path('../logs/train/multiruns/2024-03-28_11-36-50/0')\n",
    "CHECKPOINT_PATH = BASE_PATH / 'checkpoints/epoch000999.ckpt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like I don't have models saved :-D.\n",
    "\n",
    "Next steps:\n",
    "\n",
    "1. [x] Add model checkpointing w/ new config.\n",
    "2. [x] RUn a short training just to get some model to play around with.\n",
    "3. Use some of the following links to load model for inference. Links:\n",
    "    - [1](https://lightning.ai/docs/pytorch/stable/common/lightning_module.html#save-hyperparameters), [2](https://lightning.ai/docs/pytorch/stable/common/trainer.html#inference-mode), [3](https://lightning.ai/docs/pytorch/stable/deploy/production_basic.html), [4](https://lightning.ai/docs/pytorch/stable/common/checkpointing_basic.html#nn-module-from-checkpoint).\n",
    "    - G\"pytorch lightning load model for inference\"\n",
    "    - !! https://lightning.ai/docs/pytorch/stable/common/lightning_module.html#inference-in-production or\n",
    "    - https://lightning.ai/docs/pytorch/stable/common/lightning_module.html#save-hyperparameters\n",
    "    - https://lightning.ai/forums/t/save-load-model-for-inference/542\n",
    "    - https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html\n",
    "4. Then load IAMonDB dataset and do inference. Consider using `Trainer` instead of DIY approach of doing inference. That's b/c it's easier and will not be seen in the promo video anyways.\n",
    "4. Then run full training to get `checkpoint` data from there to play around.\n",
    "4. Then load handwritten X++pagewise dataset and do inference on them\n",
    "4. Then look for method to draw stroke in Jupyter Notebook.\n",
    "\n",
    "*Note:* Make sure that I do correct pre-processing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try to complete point 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/localdisk/s1691089/venvs/carbune2020/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.\n"
     ]
    }
   ],
   "source": [
    "BASE_PATH = Path('../logs/train/multiruns/2024-04-04_23-07-31/0')\n",
    "CHECKPOINT_PATH = BASE_PATH / 'checkpoints/epoch000549.ckpt'\n",
    "\n",
    "BASE_PATH = Path('../logs/train/multiruns/2024-04-06_10-29-48/0')\n",
    "CHECKPOINT_PATH = BASE_PATH / 'checkpoints/epoch000099.ckpt'\n",
    "\n",
    "BASE_PATH = Path('../logs/train/multiruns/2024-04-07_13-03-01/0')\n",
    "CHECKPOINT_PATH = BASE_PATH / 'checkpoints/epoch000649.ckpt'\n",
    "\n",
    "model = LitModule1.load_from_checkpoint(CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LitModule1(\n",
       "  (criterion): CTCLoss()\n",
       "  (log_softmax): LogSoftmax(dim=2)\n",
       "  (lstm_stack): LSTM(3, 64, num_layers=3, bidirectional=True)\n",
       "  (linear): Linear(in_features=128, out_features=82, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# - Remove other attemtps to open checkpoint\n",
    "# - Check if I am good to ignore CTCDecoder indeed\n",
    "# - tidy up code here\n",
    "# - try to do inference w/ 'iam_SimpleNormalise_xyn' model!\n",
    "\n",
    "# - OTHER: THINK ABOUT BEAM DECODER!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(CHECKPOINT_PATH, map_location=lambda storage, loc: storage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['epoch',\n",
       " 'global_step',\n",
       " 'pytorch-lightning_version',\n",
       " 'state_dict',\n",
       " 'loops',\n",
       " 'callbacks',\n",
       " 'optimizer_states',\n",
       " 'lr_schedulers',\n",
       " 'hparams_name',\n",
       " 'hyper_parameters',\n",
       " 'datamodule_hparams_name',\n",
       " 'datamodule_hyper_parameters']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(checkpoint.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = load_alphabet(BASE_PATH / 'alphabet.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet_mapper = AlphabetMapper( alphabet )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = checkpoint['hyper_parameters']['decoder']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data: From IAMOnDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/localdisk/s1691089/venvs/carbune2020/lib/python3.10/site-packages/torch/utils/data/dataset.py:414: UserWarning: Length of split at index 2 is 0. This might result in an empty dataset.\n",
      "  warnings.warn(f\"Length of split at index {i} is 0. \"\n"
     ]
    }
   ],
   "source": [
    "dm = IAMOnDBDataModule(\n",
    "    '../data/datasets/IAM-OnDB',\n",
    "    batch_size=64,\n",
    "    train_val_test_split=[0.8, 0.2, 0],\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    # limit=200,\n",
    "    limit=-1,\n",
    "    transform='iam_SimpleNormalise_xyn',\n",
    ")\n",
    "dm.setup()\n",
    "dl_val = dm.val_dataloader()\n",
    "dl_train = dm.train_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do inference: IAMOnDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = {\n",
    "    'name': [],\n",
    "    'type': [],\n",
    "    'value': [],\n",
    "}\n",
    "\n",
    "for name, dloader in {'train': dl_train, 'val': dl_val }.items():\n",
    "    for sample_batched in dloader:\n",
    "        batch = sample_batched # Just a shortcut\n",
    "\n",
    "        log_softmax = model(sample_batched['ink'].to('cuda'))\n",
    "\n",
    "        decoded_texts = decoder(log_softmax, alphabet_mapper)\n",
    "\n",
    "        # TODO: Put this in function to use both here and in model training -> or just use underlying true data\n",
    "        # TODO: Could be pre-computed (using list0 in batch to avoid endless recomputation\n",
    "        labels = []\n",
    "        for i_batch in range(log_softmax.shape[1]):\n",
    "            label_length = batch['label_lengths'][i_batch]\n",
    "            label = batch['label'][i_batch, :label_length]\n",
    "            label = [ alphabet_mapper.index_to_character(c) for c in label ]\n",
    "            label = \"\".join(label)\n",
    "            labels.append(label)\n",
    "\n",
    "        cer = char_error_rate(preds=decoded_texts, target=labels)\n",
    "        wer = word_error_rate(preds=decoded_texts, target=labels)\n",
    "\n",
    "        df_data['name'].append(name)\n",
    "        df_data['type'].append('cer')\n",
    "        df_data['value'].append(cer.item())\n",
    "\n",
    "        df_data['name'].append(name)\n",
    "        df_data['type'].append('wer')\n",
    "        df_data['value'].append(wer.item())\n",
    "\n",
    "df = pd.DataFrame.from_dict(df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">train</th>\n",
       "      <th>cer</th>\n",
       "      <td>0.052477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wer</th>\n",
       "      <td>0.235198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">val</th>\n",
       "      <th>cer</th>\n",
       "      <td>0.079784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wer</th>\n",
       "      <td>0.316895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               value\n",
       "name  type          \n",
       "train cer   0.052477\n",
       "      wer   0.235198\n",
       "val   cer   0.079784\n",
       "      wer   0.316895"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['name', 'type']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is that working indeed?!?! :-O Strangely, the train values change (both cer and wer) when re-executing but not the `val` values. Question: also when re-executing the cell?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data: From XournalPagewise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TODO: START FROM HERE NEXT TIME!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`transform` set to non-existent value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m dm \u001b[38;5;241m=\u001b[39m IAMOnDBDataModule(\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/datasets/2024-02-16-xournal_dataset.xoj\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXournalPagewise_carbune_xyn\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     10\u001b[0m )\n\u001b[0;32m---> 11\u001b[0m \u001b[43mdm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m dl_val \u001b[38;5;241m=\u001b[39m dm\u001b[38;5;241m.\u001b[39mval_dataloader()\n\u001b[1;32m     13\u001b[0m dl_train \u001b[38;5;241m=\u001b[39m dm\u001b[38;5;241m.\u001b[39mtrain_dataloader()\n",
      "File \u001b[0;32m/storage/datastore-personal/s1691089/data/code/carbune2020_implementation/src/data/online_handwriting_datamodule.py:323\u001b[0m, in \u001b[0;36mIAMOnDBDataModule.setup\u001b[0;34m(self, stage)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;241m=\u001b[39m transform\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 323\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`transform` set to non-existent value\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumber_of_channels \u001b[38;5;241m=\u001b[39m get_number_of_channels_from_dataset( \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset )\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_train, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_val, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_test \u001b[38;5;241m=\u001b[39m random_split(\n\u001b[1;32m    328\u001b[0m     dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset,\n\u001b[1;32m    329\u001b[0m     lengths\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhparams\u001b[38;5;241m.\u001b[39mtrain_val_test_split,\n\u001b[1;32m    330\u001b[0m     generator\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mGenerator()\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m42\u001b[39m),\n\u001b[1;32m    331\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: `transform` set to non-existent value"
     ]
    }
   ],
   "source": [
    "dm = IAMOnDBDataModule(\n",
    "    '../data/datasets/2024-02-16-xournal_dataset.xoj',\n",
    "    batch_size=64,\n",
    "    train_val_test_split=[0.8, 0.2, 0],\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    # limit=200,\n",
    "    limit=-1,\n",
    "    transform='XournalPagewise_carbune_xyn',\n",
    ")\n",
    "dm.setup()\n",
    "dl_val = dm.val_dataloader()\n",
    "dl_train = dm.train_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do inference: XournalPagewise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = {\n",
    "    'name': [],\n",
    "    'type': [],\n",
    "    'value': [],\n",
    "}\n",
    "\n",
    "for name, dloader in {'train': dl_train, 'val': dl_val }.items():\n",
    "    for sample_batched in dloader:\n",
    "        batch = sample_batched # Just a shortcut\n",
    "\n",
    "        log_softmax = model(sample_batched['ink'])\n",
    "\n",
    "        decoded_texts = decoder(log_softmax, alphabet_mapper)\n",
    "\n",
    "        # TODO: Put this in function to use both here and in model training -> or just use underlying true data\n",
    "        # TODO: Could be pre-computed (using list0 in batch to avoid endless recomputation\n",
    "        labels = []\n",
    "        for i_batch in range(log_softmax.shape[1]):\n",
    "            label_length = batch['label_lengths'][i_batch]\n",
    "            label = batch['label'][i_batch, :label_length]\n",
    "            label = [ alphabet_mapper.index_to_character(c) for c in label ]\n",
    "            label = \"\".join(label)\n",
    "            labels.append(label)\n",
    "\n",
    "        cer = char_error_rate(preds=decoded_texts, target=labels)\n",
    "        wer = word_error_rate(preds=decoded_texts, target=labels)\n",
    "\n",
    "        df_data['name'].append(name)\n",
    "        df_data['type'].append('cer')\n",
    "        df_data['value'].append(cer.item())\n",
    "\n",
    "        df_data['name'].append(name)\n",
    "        df_data['type'].append('wer')\n",
    "        df_data['value'].append(wer.item())\n",
    "\n",
    "df = pd.DataFrame.from_dict(df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">train</th>\n",
       "      <th>cer</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wer</th>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            value\n",
       "name  type       \n",
       "train cer     1.0\n",
       "      wer     1.4"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['name', 'type']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt n\n",
    "\n",
    "- Check point 3 references 1-4.\n",
    "- Use predict step from PL instead?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
